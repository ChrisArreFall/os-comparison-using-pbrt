This research was conducted using a robust testbench to ensure reliability and control of the variables at play. The chosen testbench was a 2019 16" MacBook Pro equipped with an Intel core i9-9980HK processor, 64 GB of RAM, and an AMD Radeon Pro 5500M GPU. To eliminate hardware discrepancies as a performance factor, both Windows and Linux were installed on the same Solid State Drive (SSD).

The experimentation involved running 16 tests on each of the five operating system scenarios: Linux installed on hardware, Linux on a Virtual Machine (VM) via Windows through VMware, Windows Subsystem for Linux (WSL), Windows, and MacOS. The 16 tests consisted of all possible combinations of the four different scenes (coffee-splash, ganesha, pbrt-book, sssdragon) and four configurations (bvh, kdtree, mitchell, sinc), resulting in a 4*4 design matrix.

Each operating system was booted individually, and the 16 scenarios were executed using an automated script. The execution order of the 16 tests was randomized for each operating system to avoid any order effects, thereby ensuring a balanced comparison across scenarios. After completing the tests on all five operating systems, the process was repeated in a different random order. This process was iterated five times.

For data collection, the output of each pbrt-v3 test was redirected to a text file. The naming scheme for these files followed the format: log-$OS-$Scene-$Config.txt, which facilitated easy identification and tracking of each test scenario.

The collected data was preprocessed using Google Sheets, converted into a .csv format, and subsequently imported into R for further analysis.

To analyze the data, a multifactorial analysis of variance (ANOVA) was performed, incorporating all three factors: Operating System, Scene, and Algorithm (Configuration). The model was implemented in R as follows:

# Multifactorial ANOVA model
model = lm(Time ~ OS*Scene*Algorithm, data=Data)

# Perform ANOVA using type II sum of squares
library(car)
Anova(model, type="II")

The ANOVA model aims to provide a comprehensive understanding of the individual and interactive effects of the operating system, scene, and configuration on the rendering time. This approach will allow for a detailed interpretation of the effects and interactions driving the performance differences between the evaluated operating system scenarios.